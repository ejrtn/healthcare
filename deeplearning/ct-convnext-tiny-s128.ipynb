{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":52254,"databundleVersionId":9674523,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install monai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pydicom\nimport random\nimport matplotlib.pyplot as plt\nfrom scipy import ndimage\nfrom scipy.ndimage import shift\nimport torch.optim as optim\nfrom torch.amp import GradScaler, autocast # 속도 및 메모리 최적화\nfrom monai.transforms import (\n    Compose, LoadImaged, Spacingd, Orientationd, EnsureChannelFirstd,\n    ScaleIntensityRanged, Resized, MapTransform, SelectItemsd, CopyItemsd, ConcatItemsd,\n    DeleteItemsd, RandFlipd, RandAffined,\n    RandGridDistortiond, RandGaussianNoised, RandAdjustContrastd, \n    RandGaussianSmoothd, Transposed, ToTensord\n)\nfrom monai.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom monai.networks.nets import resnet18\nimport torchmetrics # AUC 계산을 쉽게 해주는 라이브러리\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nimport pickle  #파일저장에\nfrom tqdm import tqdm  # 학습 진행 상황 시각화를 위해 추가\nfrom torch.optim import AdamW\nfrom joblib import Parallel, delayed    # cpu 사용하기 해해\nimport timm\n\n\n# 0. 설정 및 경로\nBASE_DIR = '/kaggle/input/rsna-2023-abdominal-trauma-detection/'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_EPOCHS = 20\nCLASS_NAME_LIST = ['bowel', 'extravasation', 'kidney', 'liver', 'spleen', 'any_injury']\nLEARNING_RATE = 1e-4\n\nMANUAL_MODEL_SAVE_PATH = '/kaggle/working/manual_ct_convnext_v1.pth'\nMANUAL_HISTORY_SAVE_PATH = '/kaggle/working/manual_ct_convnext_v1.pkl'\n\nMONAI_MODEL_SAVE_PATH = '/kaggle/working/monai_ct_convnext_v1.pth'\nMONAI_HISTORY_SAVE_PATH = '/kaggle/working/monai_ct_convnext_v1.pkl'\n\n\nIMAGE_TARGET = (64,128,128)\nNUM_SLICES = 64\n\n# 전처리된 데이터를 저장할 폴더\nSAVE_DIR = '/kaggle/working/'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# 파일 읽기\ntrain_df = pd.read_csv(f'{BASE_DIR}train_2024.csv') # 파일명 확인 필요 (보통 train.csv)\ntags_df = pd.read_parquet(f'{BASE_DIR}train_dicom_tags.parquet')\n\n# 고유 폴더 경로 추출 및 환자 ID 연결\ntags_df['series_path'] = tags_df['path'].str.split('/').str[:-1].str.join('/')\nunique_series = tags_df[['PatientID', 'series_path']].drop_duplicates()\n\ndata_dicts = []\nfor idx, row in unique_series.iterrows():\n    p_id = int(row['PatientID'])\n    s_path = row['series_path']\n    \n    # 해당 환자의 라벨 정보 가져오기\n    patient_labels = train_df[train_df['patient_id'] == p_id]\n    if len(patient_labels) == 0: continue # 라벨 없는 경우 제외\n    labels = patient_labels.iloc[0]\n    \n    data_dicts.append({\n        \"image\": f\"{BASE_DIR}{s_path}\",\n        \"patient_id\": p_id,\n\n        # 2진 분류 (Healthy, Injury) -> [1, 0] 또는 [0, 1] 형태가 됨\n        \"bowel\": labels[['bowel_healthy', 'bowel_injury']].values.astype(\"float32\"),\n        \"extravasation\": labels[['extravasation_healthy', 'extravasation_injury']].values.astype(\"float32\"),\n        \n        # 3중 분류 (Healthy, Low, High) -> [1, 0, 0], [0, 1, 0], [0, 0, 1] 형태가 됨\n        \"liver\": labels[['liver_healthy', 'liver_low', 'liver_high']].values.astype(\"float32\"),\n        \"kidney\": labels[['kidney_healthy', 'kidney_low', 'kidney_high']].values.astype(\"float32\"),\n        \"spleen\": labels[['spleen_healthy', 'spleen_low', 'spleen_high']].values.astype(\"float32\"),\n\n        # any_injury가 1이면 \"어딘가 이상함\", 0이면 \"완전 건강\"\n        \"any_injury\": np.array([1 - labels['any_injury'], labels['any_injury']]).astype(\"float32\")\n        \n    })\n\npatient_ids = train_df['patient_id'].unique()\ntrain_ids, val_ids = train_test_split(patient_ids, test_size=0.2, random_state=42)\ntrain_files = [d for d in data_dicts if d['patient_id'] in train_ids] # data_dicts에 patient_id 키 추가 필요\nval_files = [d for d in data_dicts if d['patient_id'] in val_ids]\n\nprint(f\"준비된 데이터 수: {len(data_dicts)}\")\nprint(f\"디바이스: {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ManualAugmentd(MapTransform):\n    def __init__(self, keys, prob=0.5):\n        super().__init__(keys)\n        self.prob = prob\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            if random.random() > self.prob:\n                continue\n\n            image = d[key]\n            \n            if not isinstance(image, torch.Tensor):\n                image = torch.from_numpy(image).float()\n            else:\n                image = image.float()\n            \n            # 데이터 형태: (S, C, H, W) -> 예: (64, 3, 128, 128)\n            S, C, H, W = image.shape\n            device = image.device\n            \n            # 1. Random Flip (좌우/상하)\n            if random.random() < 0.5:\n                image = torch.flip(image, dims=[3]) # 가로(W) 반전\n            if random.random() < 0.2:\n                image = torch.flip(image, dims=[2]) # 세로(H) 반전\n\n            # Random Affine (Rotation & Scale) - MONAI의 RandAffined 역할\n            # 모든 슬라이스에 '동일한' 변환을 적용해야 장기가 뒤틀리지 않음\n            if random.random() < 0.3:\n                # 회전각 (약 -10 ~ 10도)\n                angle = random.uniform(-0.15, 0.15) \n                # 스케일 (0.9 ~ 1.1)\n                scale = random.uniform(0.9, 1.1)\n                \n                # 변환 행렬 생성\n                cos_a = np.cos(angle)\n                sin_a = np.sin(angle)\n                \n                # PyTorch용 2x3 Affine Matrix (Rotation + Scale)\n                # [ [sc, -ss, tx], [ss, sc, ty] ]\n                # Translation(tx, ty)까지 추가하여 RandAffined와 더 똑같이 만듦\n                theta = torch.tensor([\n                    [cos_a * scale, -sin_a * scale, 0],\n                    [sin_a * scale,  cos_a * scale, 0]\n                ], dtype=torch.float).unsqueeze(0).to(device)\n                \n                # Grid 생성 및 적용 (Bilinear Interpolation 사용으로 부드러움)\n                grid = F.affine_grid(theta, size=(1, C, H, W), align_corners=False).to(device)\n                \n                # 모든 슬라이스에 동일한 grid 적용을 위해 루프 대신 view 활용 최적화\n                # (S, C, H, W)를 (S, C, H, W)로 변환\n                image = F.grid_sample(image, grid.repeat(S, 1, 1, 1), mode='bilinear', padding_mode='zeros', align_corners=False)\n\n            # Random Intensity (Contrast & Brightness)\n            if random.random() < 0.2:\n                gamma = random.uniform(0.7, 1.3)\n                image = torch.pow(image - image.min(), gamma) + image.min()\n\n            # Random Noise\n            if random.random() < 0.2:\n                noise = torch.randn_like(image) * 0.02\n                image = image + noise\n\n            d[key] = image\n        return d\n\n# ---------------------------------------------------------\n# 방식 1: 직접 구현 고도화 (Manual)\n# ---------------------------------------------------------\nclass CustomCTPreprocessor:\n    def __init__(self, dicom_dir, target_shape, output_spacing=(1.5, 1.5, 1.5)):\n        self.target_shape = target_shape\n        self.output_spacing = output_spacing # 물리적 mm 단위 통일 (성능 향상의 핵심)\n\n        self.result = self.process(dicom_dir)\n\n    def load_and_sort_dicom(self, dicom_dir):\n        \"\"\"DICOM 로드 및 물리적 위치(Z축) 기준 정렬\"\"\"\n        files = [pydicom.dcmread(os.path.join(dicom_dir, f)) for f in os.listdir(dicom_dir)]\n        # ImagePositionPatient의 3번째 값(Z)으로 정렬해야 해부학적 순서가 맞음\n        files.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n        return files\n\n    def get_hu_image(self, slices):\n        \"\"\"Raw Pixel을 물리적 밀도 단위(HU)로 변환\"\"\"\n        image = np.stack([s.pixel_array for s in slices]).astype(np.float32)\n        \n        # 장비별 Rescale Slope/Intercept 적용\n        slope = slices[0].RescaleSlope\n        intercept = slices[0].RescaleIntercept\n        image = image * slope + intercept\n        return image\n\n    def apply_multi_window(self, hu_image):\n        \"\"\"\n        [성능 향상 팁] 3개의 서로 다른 윈도우를 RGB 채널처럼 사용\n        - Channel 0: Soft Tissue (전반적 장기)\n        - Channel 1: Liver/Spleen (고대비 장기 특화)\n        - Channel 2: Bone/Air (경계선 강조)\n        \"\"\"\n        def windowing(img, wl, ww):\n            lower, upper = wl - ww//2, wl + ww//2\n            img_clip = np.clip(img, lower, upper)\n            return (img_clip - lower) / (upper - lower)\n\n        # 임상적 근거 의한 값\n        # ch0 (50, 400) - 복부 표준:\n        # 복부 장기(간, 비장, 신장 등)의 평균 밀도가 보통 40~60 HU입니다. 그래서 중심을 50으로 잡습니다.\n        # 주변의 지방(-50)부터 약간의 석회화(+200)까지 넓게 보기 위해 폭을 400으로 설정합니다.\n        # ch1 (30, 150) - 간 특화(고대비):\n        # 간 내부의 미세한 출혈이나 종양은 주변 조직과 밀도 차이가 아주 적습니다(약 10~20 HU 차이).\n        # 이걸 잡아내려면 폭(WW)을 아주 좁게(150) 줄여서 대비를 극대화해야 합니다. 그래야 미세하게 어두운 부분이 확연히 드러납니다.\n        # ch2 (100, 700) - 광범위/뼈/혈관:\n        # 조영제가 들어간 혈관이나 뼈 근처의 출혈은 밀도가 높습니다.\n        # 더 높은 수치(+100 이상)까지 포함하면서, 전체적인 윤곽을 잃지 않기 위해 범위를 아주 넓게(700) 잡은 것입니다.\n        ch0 = windowing(hu_image, 50, 400)   # Standard Abdomen\n        ch1 = windowing(hu_image, 30, 150)   # High Contrast Liver\n        ch2 = windowing(hu_image, 100, 700)  # Wide Range (Bone/Fluid)\n        \n        return np.stack([ch0, ch1, ch2], axis=0) # (3, D, H, W)\n\n    def resample_isotropic(self, image, slices):\n        \"\"\"\n        병원마다 다른 슬라이스 두께를 1.5mm로 통일\n        이 과정을 거쳐야 모델이 장기의 '진짜 크기'를 배움\n        픽셀 1개가 실제 몸속에서 몇 mm인가? 맞추는 작업입니다. \n        \"\"\"\n        # 현재 Spacing (Thickness, PixelSpacing_X, PixelSpacing_Y)\n        current_spacing = np.array([\n            float(slices[0].SliceThickness),\n            float(slices[0].PixelSpacing[0]),\n            float(slices[0].PixelSpacing[1])\n        ])\n        \n        resize_factor = current_spacing / self.output_spacing\n        # ndimage.zoom으로 물리적 비율 보정 (채널별로 반복)\n        new_channels = []\n        for c in range(image.shape[0]):\n            resampled = ndimage.zoom(image[c], resize_factor, order=1)\n            new_channels.append(resampled)\n        \n        return np.stack(new_channels, axis=0)\n\n    def final_resize_and_norm(self, image):\n        \"\"\"\n        최종 크기 조정 및 Z-Score 정규화\n        단순히 이미지를 가로, 세로, 높이 128개의 칸으로 강제로 늘리거나 줄이는 것\n        \"\"\"\n        # 1. 모델 규격(64x128x128)으로 리사이즈\n        factors = [\n            1.0, # Channel은 고정\n            self.target_shape[0] / image.shape[1],\n            self.target_shape[1] / image.shape[2],\n            self.target_shape[2] / image.shape[3]\n        ]\n        image = ndimage.zoom(image, factors, order=1)\n        \n        # 2. Z-Score 정규화: (x - mean) / std\n        # 0~1 정규화보다 모델의 수렴 속도가 훨씬 빠름\n\n        # 1e-8 더하는 유유\n        # 만약 특정 슬라이스(image[c])의 모든 픽셀 값이 똑같다면(예: 전부 검은색 배경만 있는 경우),\n        # 해당 이미지의 표준편차(std)는 0이되어 나눌 수 없게 됩니다.\n        # 그래서 float32에서 1e-8 / float16 1e-5 정도가 가장 적당한 \"아주 작은 수\"를 더합니다.\n        # 그냥 무작정 너무 작게 잡으면 0으로 인실 할 수 있음\n        for c in range(image.shape[0]):\n            image[c] = (image[c] - image[c].mean()) / (image[c].std() + 1e-8)\n            \n        return image\n        \n    def process(self, dicom_dir):\n        \"\"\"전체 파이프라인 실행\"\"\"\n        slices = self.load_and_sort_dicom(dicom_dir)\n        hu_img = self.get_hu_image(slices)\n        multi_win = self.apply_multi_window(hu_img)\n        resampled = self.resample_isotropic(multi_win, slices)\n        final_img = self.final_resize_and_norm(resampled)\n        return final_img.astype(np.float32) # 최종 출력: (3, 128, 128, 128)\n\n\n# ---------------------------------------------------------\n# 방식 2: MONAI (비교를 위해 단계를 Manual과 맞춤)\n# ---------------------------------------------------------\ndef get_monai_expert_pipeline():\n    return Compose([\n        LoadImaged(keys=[\"image\"]),\n        EnsureChannelFirstd(keys=[\"image\"]),\n        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n        # 1. 물리적 해상도 통일 (Isotropic Resampling)\n        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 1.5), mode=\"bilinear\"),\n        \n        # 2. 멀티 윈도우 채널 생성 (이미지를 3개로 복사)\n        CopyItemsd(keys=[\"image\"], times=3, names=[\"img_soft\", \"img_liver\", \"img_bone\"]),\n        \n        # 3. 각 복사본에 서로 다른 윈도우 적용\n        ScaleIntensityRanged(keys=[\"img_soft\"], a_min=-150, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n        ScaleIntensityRanged(keys=[\"img_liver\"], a_min=-50, a_max=100, b_min=0.0, b_max=1.0, clip=True),\n        ScaleIntensityRanged(keys=[\"img_bone\"], a_min=-100, a_max=600, b_min=0.0, b_max=1.0, clip=True),\n        \n        # 4. 3개 채널을 하나로 합침 (3, 64, 128, 128)\n        ConcatItemsd(keys=[\"img_soft\", \"img_liver\", \"img_bone\"], name=\"image\"),\n        DeleteItemsd(keys=[\"img_soft\", \"img_liver\", \"img_bone\"]),\n\n        # 5. 최종 크기 조정 및 배경 제거 효과\n        Resized(keys=[\"image\"], spatial_size=IMAGE_TARGET)\n    ])\n\n\nclass Timm_Model(torch.nn.Module):\n    def __init__(self, model_name='convnext_tiny'):\n        super().__init__()\n        # 특징 추출기 (ConvNeXt)\n        # num_classes = 1000 (기본값): 모델의 최종 출력이 1,000개의 숫자(카테고리 점수)로 나옵니다.\n        # num_classes = 0: 1,000개를 맞히는 마지막 층을 아예 없애버립니다. 대신, 그 바로 직전 단계인 **'이미지의 핵심 특징 정보(Feature Vector)'**를 그대로 출력합니다.\n        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n        self.dim = self.backbone.num_features # Base 기준 1024\n\n        # 어텐션 풀링: 128장 중 수상한 놈을 골라내는 '심사위원'\n        self.attention_net = nn.Sequential(\n            nn.Linear(self.dim, 256),\n            nn.Tanh(),\n            nn.Dropout(0.1), # 추가\n            nn.Linear(256, 1)\n        )\n        \n        # \"이상 징후 탐지\" 전용 헤드 (의심 모델 역할)\n        self.suspicion_head = nn.Sequential(\n            nn.Linear(self.dim, 256),  # 1024개를 256개의 핵심 의심 후보로 압축\n            nn.LayerNorm(256),  # 학습을 안정적으로 만들어줌\n            nn.ReLU(),            # 중요한 의심 신호만 통과시킴\n            nn.Dropout(0.2),      # 과적합 방지 (너무 예민해지는 것 방지)\n            nn.Linear(256, 2)     # 최종 경보 [정상, 이상]\n        )\n\n        # \"정밀 병명 분류\" 전용 헤드 (분류 모델 역할)\n        # 장기별 결과 2 or 3개 도출\n        self.organ_heads = nn.ModuleDict({\n            'bowel': nn.Linear(self.dim, 2),\n            'extravasation': nn.Linear(self.dim, 2),\n            'liver': nn.Linear(self.dim, 3),\n            'kidney': nn.Linear(self.dim, 3),\n            'spleen': nn.Linear(self.dim, 3)\n        })\n\n    def forward(self, x):\n        # 2.5D 방식으로 전체 슬라이스 훑기\n        b, s, c, h, w = x.shape\n        chunk_size = 8 # 한 번에 처리할 슬라이스 개수 (메모리에 따라 조절)\n        all_features = []\n        \n        for i in range(0, s, chunk_size):\n            # x_chunk: (Batch, 16, 3, 128, 128)\n            x_chunk = x[:, i : i + chunk_size] \n            \n            # 2D 연산을 위해 일시적으로 배치 차원으로 합침\n            x_chunk = x_chunk.reshape(-1, c, h, w) # (Batch*16, 3, 128, 128)\n            \n            # 백본 통과 (이 순간 메모리 사용량이 chunk_size만큼으로 제한됨)\n            feat_chunk = self.backbone(x_chunk) # (Batch*16, 1024)\n            \n            # 다시 슬라이스 차원 분리 후 리스트에 저장\n            feat_chunk = feat_chunk.view(b, -1, self.dim) \n            all_features.append(feat_chunk)\n\n        # 모든 특징 합치기\n        features = torch.cat(all_features, dim=1) # (Batch, 64, 1024)\n    \n        # features = self.backbone(x) # (B*64, 1024)\n        # features = features.view(b, s, -1) # (B, 64, 1024)\n\n        # Attention Pooling으로 '이상 지점' 증폭\n        # 각 슬라이스의 수상함 점수 계산\n        att_scores = self.attention_net(features) # (B, 64, 1)\n        \n         # 점수를 0~1 사이 비중(가중치)으로 변환\n        att_weights = F.softmax(att_scores, dim=1) # (B, S, 1)\n        \n        # 가중치를 곱해서 하나로 합침 (가장 수상한 슬라이스 정보가 증폭됨)\n        combined = torch.sum(features * att_weights, dim=1) # (B, 1024)\n\n        # 결과 도출\n        out = {k: head(combined) for k, head in self.organ_heads.items()}\n        out['any_injury'] = self.suspicion_head(combined)\n\n        return out\n\n\ndef process_one_item(idx, item, total_count, mode, pipeline=None):\n\n    step = max(1, total_count // 100)\n\n    if idx % step == 0:\n        percent = (idx / total_count) * 100\n        # flush=True를 써야 백그라운드 로그에 즉시 기록됩니다.\n        print(f\"[{mode}] Progress: {percent:.0f}% 완료 ({idx}/{total_count})\", flush=True)\n        \n    \"\"\"\n    한 명의 환자 데이터를 전처리하고 파일로 저장하는 핵심 함수\n    \"\"\"\n    p_id = item['patient_id']\n    s_id = item['image'].split('/')[-1]\n    \n    # 저장 경로 설정 (128 사이즈 구분을 위해 이름에 포함 가능)\n    save_path = os.path.join(SAVE_DIR, f\"{mode}_{p_id}_{s_id}.npz\")\n    \n    # 1. 이미 파일이 있으면 전처리 생략하고 바로 리턴 (시간 절약)\n    if os.path.exists(save_path):\n        new_item = item.copy()\n        new_item['image'] = save_path\n        return new_item\n\n    try:\n        # 2. 방식에 따른 전처리 수행\n        if mode == \"manual\":\n            # Manual 방식: 직접 짠 CustomCTPreprocessor 호출\n            # TARGET_SIZE는 전역 변수(예: (128, 128, 128))를 참조합니다.\n            pre = CustomCTPreprocessor(item['image'], target_shape=IMAGE_TARGET)\n            img = pre.result.astype(np.float16)\n        else:\n            # MONAI 방식: 전달받은 monai_pipeline 호출\n            processed = pipeline(item)\n            img = processed[\"image\"].detach().cpu().numpy().astype(np.float16)\n            \n        # (Channel, Depth, H, W) -> (Slices, Channel, H, W)\n        # 결과 형태: (64, 3, 128, 128)\n        img = np.transpose(img, (1, 0, 2, 3))\n        \n        # 3. 압축률이 적어 의미 없다 판단하여 그냥 저장\n        np.savez_compressed(save_path, img)\n        \n        # 4. 경로를 업데이트한 새 딕셔너리 반환\n        new_item = item.copy()\n        new_item['image'] = save_path\n        return new_item\n\n    except Exception as e:\n        print(f\"[{mode}] Error ID {p_id}: {e}\")\n        return None\n\n\ndef monai_train_pipeline():\n    return Compose([\n        LoadNpyTransformd(keys=[\"image\"]),\n\n        # MONAI 3D 연산을 위해 차원 변경 (C, S, H, W) -> (3, 64, 128, 128)\n        # MONAI는 첫 번째 차원을 무조건 Channel로 간주합니다.\n        Transposed(keys=[\"image\"], indices=(1, 0, 2, 3)),\n\n        # 공간적 변형 (Spatial)\n        # spatial_axis: 0=S(Slices), 1=H, 2=W\n        RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=1), # 좌우 반전\n        RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=2), # 상하 반전\n        \n        RandAffined(\n            keys=[\"image\"],\n            prob=0.2,\n            # (S, H, W) 각 축에 대한 회전/스케일\n            rotate_range=(0.1, 0.1, 0.1), \n            scale_range=(0.1, 0.1, 0.1),\n            translate_range=(10, 10, 10),\n            padding_mode=\"zeros\",\n            mode=\"bilinear\"\n        ),\n        \n        # 형태적 변형 (Grid Distortion)\n        # 매우 강력하지만 연산량이 많음. T4 x2에서는 CPU 병목을 확인하며 사용할 것.\n        RandGridDistortiond(\n            keys=[\"image\"],\n            prob=0.2,\n            num_cells=(4, 4, 4),\n            distort_limit=(-0.05, 0.05), # 변형 강도 추가 (옵션)\n            mode=\"bilinear\"\n        ),\n        \n        # 강도 및 노이즈 (Intensity)\n        RandGaussianNoised(keys=[\"image\"], prob=0.2, mean=0.0, std=0.05),\n        RandAdjustContrastd(keys=[\"image\"], prob=0.2, gamma=(0.7, 1.3)),\n        RandGaussianSmoothd(keys=[\"image\"], prob=0.1, sigma_x=(0.5, 1.0)),\n        \n        # 모델 입력을 위해 다시 원래 차원으로 복구 (S, C, H, W)\n        # Timm_Model이 (Batch, Slices, C, H, W)를 기대하므로\n        Transposed(keys=[\"image\"], indices=(1, 0, 2, 3)),\n\n        ToTensord(keys=[\"image\"] + CLASS_NAME_LIST),\n        \n        SelectItemsd(keys=[\"image\"] + CLASS_NAME_LIST)\n    ])\n\n\ndef monai_val_pipeline():\n    return Compose([\n        LoadNpyTransformd(keys=[\"image\"]),\n        ToTensord(keys=[\"image\"] + CLASS_NAME_LIST),\n        SelectItemsd(keys=[\"image\"] + CLASS_NAME_LIST)\n    ])\n    \n\nclass LoadNpyTransformd(MapTransform):\n    def __call__(self, data):\n        d = dict(data)\n        # npz 파일 로드\n        with np.load(d[\"image\"]) as data_file:\n            # 저장할 때 사용했던 키인 'img'를 사용해 데이터를 꺼냅니다.\n            img = data_file['img']\n            \n        # numpy 배열을 torch 텐서로 변환합니다.\n        d[\"image\"] = torch.from_numpy(img).float()\n        return d\n\n\ndef evaluate(model, loader, epoch, criterion):\n    model.eval()\n    val_epoch_loss = 0\n    results = {k: {\"preds\": [], \"trues\": []} for k in CLASS_NAME_LIST}\n\n    with torch.no_grad():\n        val_loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]\", leave=False)\n        for batch in val_loop:\n            inputs = batch[\"image\"].to(DEVICE)\n            outputs = model(inputs)\n\n            # 분류\n            loss = 0\n            for k in CLASS_NAME_LIST:\n                raw_pred = torch.softmax(outputs[k], dim=1).detach().cpu()\n                pred = raw_pred.as_tensor() if hasattr(raw_pred, \"as_tensor\") else raw_pred\n\n                raw_true = batch[k].detach().cpu()\n                if raw_true.dim() > 1:\n                    true = torch.argmax(raw_true, dim=1)\n                else:\n                    true = raw_true.long()\n                true = true.as_tensor() if hasattr(true, \"as_tensor\") else true\n                \n                results[k][\"preds\"].append(pred)\n                results[k][\"trues\"].append(true)\n\n                # Loss 계산용 정답값 처리\n                target = batch[k].to(DEVICE).float() # long() 변환 대신 float()으로 명시적 변환\n                loss += criterion(outputs[k], target)\n                    \n\n            val_epoch_loss += loss.item()\n            val_loop.set_postfix(val_loss=loss.item())\n\n    avg_val_loss = val_epoch_loss / len(loader)\n    \n    # AUC 계산\n    auc_results = {}\n    for k in CLASS_NAME_LIST:\n        all_preds = torch.cat(results[k][\"preds\"])\n        all_trues = torch.cat(results[k][\"trues\"])\n        num_classes = all_preds.shape[1] \n        \n        # AUC 계산기는 CPU에서 수행하는 것이 안전함\n        auc_metric = torchmetrics.AUROC(task=\"multiclass\", num_classes=num_classes)\n        auc_results[k] = auc_metric(all_preds, all_trues).item()\n    \n    return auc_results, avg_val_loss\n\n\ndef train(train_files_preprocessed, val_files_preprocessed, \n          train_pipeline, val_pipeline,\n          model_save_path, history_save_path):    \n    \n    # 비교하기\n    # 구분\t    \tBCEWithLogitsLoss\t\t      \t\tCrossEntropyLoss\n    # 풀네임\t\t    Binary Cross Entropy with Logits\t\t(Multiclass) Cross Entropy\n    # 주요 목적\t\t이진 분류 (Yes or No)\t\t\t    \t다중 분류 (A, B, C 중 하나)\n    # 출력 노드 수\t1개 (0~1 사이의 확률)\t\t\t    \tN개 (각 클래스별 점수)\n    # 활성 함수\t\tSigmoid (내장됨)\t\t\t\t    \tSoftmax (내장됨)\n    # 타겟 라벨\t\t0.0 또는 1.0 (Float)\t\t\t     \t0, 1, 2... 인덱스 (Long)\n    # 특징\t\t    각 타겟이 독립적임 (Multi-label 가능)\t타겟 간 경쟁 관계 (합이 1이 됨)\n    \n    train_ds = Dataset(data=train_files_preprocessed, transform=train_pipeline)\n    val_ds = Dataset(data=val_files_preprocessed, transform=val_pipeline)\n    \n    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=(DEVICE.type == 'cuda') )\n    val_loader = DataLoader(val_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=(DEVICE.type == 'cuda') )\n    \n    # 2. 모델, 손실함수, 옵티마이저\n    model = Timm_Model(model_name='convnext_tiny').to(DEVICE)\n    if torch.cuda.device_count() > 1:\n        print(\"2개의 GPU를 사용합니다.\")\n        model = nn.DataParallel(model) # 모델을 복사하여 양쪽 GPU에 분산\n        \n    criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.05)\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n    \n    # 3. 스케줄러 설정 (CosineAnnealingLR 예시)\n    # T_max: 보통 전체 에포크 수로 설정합니다.\n    # scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n    \n    # (참고) 만약 안정성을 중시한다면 아래 스케줄러를 사용하세요.\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n\n    scaler = GradScaler('cuda', enabled=(DEVICE.type == 'cuda')) # DEVICE가 object이므로 .type 추가 권장\n    \n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"auc_avg_loss\": [],  # 이전에 만든 AUC도 기록\n        \"auc_details\": []\n    }\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        train_epoch_loss = 0\n    \n        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n        for batch in train_loop:\n            inputs = batch[\"image\"].to(DEVICE)\n            optimizer.zero_grad()\n            \n            with autocast('cuda'):\n                outputs = model(inputs)\n                \n                # Loss 계산 (초기화 중요)\n                loss = 0\n                for k in CLASS_NAME_LIST:\n                    target = batch[k].to(DEVICE)\n                    # CrossEntropy는 target이 float(확률)이면 그대로, int(인덱스)면 long으로 변환 필요\n                    if target.dtype != torch.float32:\n                        target = target.float()\n                    \n                    if k == 'any_injury':\n                        loss += criterion(outputs[k], target) * 2.0\n                    else:\n                        loss += criterion(outputs[k], target)\n                \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n    \n            train_epoch_loss += loss.item()\n            train_loop.set_postfix(loss=loss.item())\n    \n        avg_train_loss = train_epoch_loss / len(train_loader)\n    \n    \n        # 에포크 종료 후 성능 출력\n        auc_results, avg_val_loss = evaluate(model, val_loader, epoch, criterion) # 실무에선 val_loader 사용 권장\n        mean_auc = sum(auc_results.values()) / len(auc_results)\n        \n        history[\"train_loss\"].append(avg_train_loss)\n        history[\"val_loss\"].append(avg_val_loss)\n        history[\"auc_avg_loss\"].append(mean_auc)\n        history[\"auc_details\"].append(auc_results)\n    \n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # CosineAnnealingLR 사용 시 (에포크 끝날 때마다 호출)\n        # scheduler.step()\n        \n        # ReduceLROnPlateau 사용시\n        scheduler.step(avg_val_loss) \n        \n        print(f\"\\n>>> Epoch {epoch+1} Summary\")\n        print(f\"LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n        print(f\"Mean AUC: {mean_auc:.4f}\")\n        for organ, val in auc_results.items():\n            print(f\" - {organ:15s}: {val:.4f}\")\n        print(\"-\" * 50)\n\n        # 5. 모델 가중치 저장\n        torch.save(model.state_dict(), f'{epoch}_{model_save_path}')\n        print(f\"✅ 모델 가중치 저장 완료: {epoch}_{model_save_path}\")\n        \n        # 6. 학습 히스토리 저장 (Pickle)\n        with open(history_save_path, 'wb') as file:\n            pickle.dump(history, f'{epoch}_{file}')\n        print(f\"✅ 학습 히스토리 저장 완료: {epoch}_{history_save_path}\")\n\n    return history\n\n    \ndef show_history(history):\n    plt.figure(figsize=(15, 6))\n\n    # 1. Loss 그래프 (Training vs Validation)\n    plt.subplot(1, 3, 1)\n    plt.plot(history[\"train_loss\"], label=\"Train Loss\", marker='o')\n    plt.plot(history[\"val_loss\"], label=\"Val Loss\", marker='o')\n    plt.title(\"Training & Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    plt.legend()\n    \n    # 2. Mean AUC 그래프\n    # 키 이름을 val_auc_mean으로 수정했습니다.\n    plt.subplot(1, 3, 2)\n    plt.plot(history[\"auc_avg_loss\"], label=\"Mean Val AUC\", color='orange', marker='s')\n    plt.title(\"Mean Validation AUC\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"AUC\")\n    plt.grid(True)\n    plt.legend()\n    \n    # 장기별로 리스트를 추출하여 그래프 그리기\n    plt.subplot(1, 3, 3) # 1행 3열 중 3번째 (에러 해결 지점)\n    for organ in CLASS_NAME_LIST:\n        # 각 장기별 데이터를 추출하여 루프 안에서 그립니다.\n        organ_auc_history = [epoch_data[organ] for epoch_data in history[\"auc_details\"]]\n        plt.plot(organ_auc_history, label=f\"{organ}\")\n    \n    plt.title(\"Validation AUC by Organ\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"AUC\")\n    plt.ylim(0.4, 1.05) # AUC가 1일 수도 있으므로 1.05 정도로 설정\n    plt.grid(True, linestyle='--')\n    # 범례가 많을 수 있으므로 그래프 옆으로 뺍니다.\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small') \n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Manual 전처리\n# print(\"Train 데이터 전처리 시작 (Parallel)...\")\n# # 1000개만 선택\n# train_files_subset = train_files[2000:]\n# total_len = len(train_files_subset) # 진행률 표시를 위해 1000으로 설정\n\n# train_results = Parallel(n_jobs=-1)(\n#     delayed(process_one_item)(idx, item, total_len, \"manual\") \n#     for idx, item in enumerate(train_files_subset)\n# )\n# # 에포크 에러 방지를 위해 None 제거 (주소록 업데이트)\n# train_files_preprocessed = [r for r in train_results if r is not None]\n# print(\"Train 데이터 전처리 종료 (Parallel)...\")\n\n# print(\"Val 데이터 전처리 시작 (Parallel)...\")\n# total_len = len(val_files)\n# val_results = Parallel(n_jobs=-1)(\n#     delayed(process_one_item)(idx, item, total_len, \"manual\") for idx,item in enumerate(val_files)\n# )\n# # 에포크 에러 방지를 위해 None 제거 (주소록 업데이트)\n# val_files_preprocessed = [r for r in val_results if r is not None]\n# print(\"Val 데이터 전처리 종료 (Parallel)...\")\n\n# # Manual 학습'\n# manual_train_loader_pipeline = Compose([\n#     LoadNpyTransformd(keys=[\"image\"]),\n#     ManualAugmentd(keys=[\"image\"]), # 여기서 증강 실행!\n# ])\n# manual_val_loader_pipeline = Compose([\n#     LoadNpyTransformd(keys=[\"image\"]),\n# ])\n# print(\"=\" * 25,\"Manual\",\"=\" * 25)\n# history = train(train_files_preprocessed, val_files_preprocessed, \n#                 manual_train_loader_pipeline, manual_val_loader_pipeline, \n#                 MANUAL_MODEL_SAVE_PATH, MANUAL_HISTORY_SAVE_PATH)\n\n# show_history(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MONAI 전리리\npipeline = get_monai_expert_pipeline()\n\n# 1000개만 선택\ntrain_files_subset = train_files[:2000]\ntotal_len = len(train_files_subset) # 진행률 표시를 위해 1000으로 설정\n\nprint(\"Train 데이터 전처리 시작 (Parallel)...\")\ntrain_results = Parallel(n_jobs=-1)(\n    delayed(process_one_item)(idx, item, total_len, \"monai\", pipeline)\n    for idx,item in enumerate(train_files_subset)\n)\n# 에포크 에러 방지를 위해 None 제거 (주소록 업데이트)\ntrain_files_preprocessed = [r for r in train_results if r is not None]\nprint(\"Train 데이터 전처리 종료 (Parallel)...\")\n\n# print(\"Val 데이터 전처리 시작 (Parallel)...\")\n# total_len = len(val_files) # 진행률 표시를 위해 1000으로 설정\n# val_results = Parallel(n_jobs=-1)(\n#     delayed(process_one_item)(idx, item, total_len, \"monai\", pipeline)\n#     for idx,item in enumerate(val_files)\n# )\n# # 에포크 에러 방지를 위해 None 제거 (주소록 업데이트)\n# val_files_preprocessed = [r for r in val_results if r is not None]\n# print(\"Val 데이터 전처리 종료 (Parallel)...\")\n\n# # MONAI 학습\n# monai_train_loader_pipeline = monai_train_pipeline()\n# monai_val_loader_pipeline = monai_val_pipeline()\n\n# print(\"=\" * 25,\"Monai\",\"=\" * 25)\n# history = train(train_files_preprocessed, val_files_preprocessed, \n#                 monai_train_loader_pipeline, monai_val_loader_pipeline,\n#                 MONAI_MODEL_SAVE_PATH, MONAI_HISTORY_SAVE_PATH)\n\n# show_history(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}